# -*- coding: utf-8 -*-
"""DeepLearningwithTensorFlowandKeras:CourseEndProjectsV3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13yA7oj-B65r3Hek-7DItH8QVTrVzOgkb

# Deep Learning with TensorFlow and Keras: Course End Projects

https://drive.google.com/drive/folders/1kwwdDkY5QYqpS_OwB7DOm1GJ7Mox92nb

Carllos Watts-Nogueira

Due: Jul 12 by 12:59am

# Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, recall_score, classification_report, roc_curve, roc_auc_score

from sklearn.utils import resample

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.metrics import AUC

from keras.callbacks import EarlyStopping

"""# 1. and 2. Load dataset / Check for null values in the dataset"""

# Load the Data Dic
df_dd = pd.read_csv('Data_Dictionary.csv')
df_dd

df_dd.info()

# Load dataset
df = pd.read_csv('loan_data.csv')

# copy df
df_main_cpy = df.copy(deep=True)

df_main_cpy.info()

df.info()

df.head()

df.isnull().sum()

# dtype
df.dtypes

# new df
df_info = pd.DataFrame({
    'Column Name': df.columns,
    'Data Type': df.dtypes.values,
    'Missing Values': df.isnull().sum().values,
    'Missing %': (df.isnull().mean().values * 100).round(2)
})

# mix by column name
summary_df = pd.merge(df_info, df_dd, how='left', left_on='Column Name', right_on='Variable Name')

# Select columns, rename
summary_df = summary_df[['Column Name', 'Description', 'Data Type', 'Missing Values', 'Missing %']]

# show all rows
pd.set_option('display.max_rows', None)

# print
summary_df

summary_df[summary_df['Data Type'] == 'object']

# Cria uma cópia do dataframe apenas com as colunas do tipo 'object'
object_summary = summary_df[summary_df['Data Type'].astype(str) == 'object'].copy()

# Adiciona a nova coluna com os valores únicos de cada coluna
object_summary['Unique Values'] = object_summary['Column Name'].apply(lambda col: df[col].dropna().unique().tolist())

object_summary

summary_df[summary_df['Missing %'] > 50].sort_values('Missing %', ascending=False)

print(summary_df['Data Type'].unique())

summary_df[summary_df['Data Type'].astype(str).isin(['int64', 'float64'])]

nulls = df.isnull().sum()
print(nulls[nulls > 0])

medium_missing = nulls[(nulls > 0) & (nulls < 41519)]
print(medium_missing)

high_missing = nulls[nulls > 41519]
print(high_missing)

# copy df
df_main = df.copy(deep=True)

"""# Data Cleaning + Feature Engineering"""

# Variables with few missing values (up to 1%)

# Simple imputation for low-missing variables
low_nulls = ['CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',
             'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH',
             'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH',
             'CNT_FAM_MEMBERS']

for col in low_nulls:
    df[col] = df[col].fillna(df[col].median())

# variables with moderate missing rates (10-20%)

# Impute EXT_SOURCE columns with mean
ext_sources = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']
for col in ext_sources:
    df[col] = df[col].fillna(df[col].mean())

# Create missing flags for bureau-related features
bureau_cols = [col for col in df.columns if 'AMT_REQ_CREDIT_BUREAU' in col]
for col in bureau_cols:
    df[col + '_missing_flag'] = df[col].isnull().astype(int)
    df[col] = df[col].fillna(0)

# Structural/physical variables (~50–70% missing)

# Identify numeric columns with more than 50% missing values
high_missing_numeric = [
    col for col in df.select_dtypes(include=['number']).columns
    if df[col].isnull().mean() > 0.5
]

# Efficient creation of missing flags
missing_flags = {
    col + '_missing_flag': df[col].isnull().astype(int)
    for col in high_missing_numeric
}

# Concatenate all flags into the main DataFrame
df = pd.concat([df, pd.DataFrame(missing_flags)], axis=1)

# Impute missing values using the median
for col in high_missing_numeric:
    df[col] = df[col].fillna(df[col].median())

# Normalize strings: categorical columns with no missing values
categorical_cols = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']
for col in categorical_cols:
    df[col] = df[col].str.lower().str.strip()

# Impute mode for columns with low missingness
light_nulls = [
    'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',
    'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'WEEKDAY_APPR_PROCESS_START',
    'ORGANIZATION_TYPE'
]
for col in light_nulls:
    df[col] = df[col].fillna(df[col].mode()[0])

# Impute 'Unknown' for categorical columns with high missingness
heavy_nulls = [
    'OCCUPATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE',
    'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE'
]
for col in heavy_nulls:
    df[col + '_missing_flag'] = df[col].isnull().astype(int)
    df[col] = df[col].fillna('Unknown')

# new df
df_infotw = pd.DataFrame({
    'Column Name': df.columns,
    'Data Type': df.dtypes.values,
    'Missing Values': df.isnull().sum().values,
    'Missing %': (df.isnull().mean().values * 100).round(2)
})

# mix dic by colum name
summary_df = pd.merge(df_infotw, df_dd, how='left', left_on='Column Name', right_on='Variable Name')

# rename
summary_df = summary_df[['Column Name', 'Description', 'Data Type', 'Missing Values', 'Missing %']]

# all rows
pd.set_option('display.max_rows', None)

#
summary_df

# Identify columns where nulls are still present
null_summary = df.isnull().sum()
remaining_nulls = null_summary[null_summary > 0]
print(remaining_nulls)

low_nulls = df.columns[df.isnull().sum() <= 2].tolist()

for col in low_nulls:
    if pd.api.types.is_numeric_dtype(df[col]):
        df[col] = df[col].fillna(df[col].median())
    else:
        df[col] = df[col].fillna(df[col].mode()[0])

# List of relevant columns to treat
relevant_nulls = [
    'YEARS_BEGINEXPLUATATION_AVG', 'FLOORSMAX_AVG',
    'YEARS_BEGINEXPLUATATION_MODE', 'FLOORSMAX_MODE',
    'YEARS_BEGINEXPLUATATION_MEDI', 'FLOORSMAX_MEDI',
    'TOTALAREA_MODE', 'OBS_30_CNT_SOCIAL_CIRCLE',
    'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',
    'DEF_60_CNT_SOCIAL_CIRCLE'
]

# Create missing flags efficiently
missing_flags = {
    col + '_missing_flag': df[col].isnull().astype(int)
    for col in relevant_nulls
}

# Add flags to the main DataFrame
df = pd.concat([df, pd.DataFrame(missing_flags)], axis=1)

# Impute based on column type
for col in relevant_nulls:
    if pd.api.types.is_numeric_dtype(df[col]):
        df[col] = df[col].fillna(df[col].median())
    else:
        df[col] = df[col].fillna(df[col].mode(dropna=True)[0])

print("Remaining nulls:", df.isnull().sum().sum())

#
df_info = pd.DataFrame({
    'Column Name': df.columns,
    'Data Type': df.dtypes.values,
    'Missing Values': df.isnull().sum().values,
    'Missing %': (df.isnull().mean().values * 100).round(2)
})

#
summary_df = pd.merge(df_info, df_dd, how='left', left_on='Column Name', right_on='Variable Name')

#
summary_df = summary_df[['Column Name', 'Description', 'Data Type', 'Missing Values', 'Missing %']]

#
pd.set_option('display.max_rows', None)

#
summary_df

df = df.drop(columns=['SK_ID_CURR'])

# Drop duplicate or malformed flag columns
columns_to_drop = [col for col in df.columns if 'missing_flag_missing_flag' in col]
df = df.drop(columns=columns_to_drop)

missing_percentage = df.isnull().mean() * 100
missing_percentage

# df.dropna(inplace=True)

df.info()

# is ok I deleted 298.909 is ok? or better dropna() columns more afected with nulls and latter dropna rows?

df.columns

"""# 3. Print the percentage of default to a payer of the dataset for the TARGET column"""

# Option A, with mean()
# default_rate = df['TARGET'].mean()
# print(f"Default percentage: {default_rate * 100:.2f}%")

# Option B, with value_counts and normalize
default_rate = df['TARGET'].value_counts(normalize=True)[1] * 100
print(f"Default percentage: {default_rate:.2f}%")

"""# 4. Balance the dataset if the data is imbalanced"""

# data is imbalanced?
sns.countplot(x='TARGET', data=df)
plt.title('Default vs Non-default Counts')
plt.show()

# If the data is more "0" non-defaults than "1" defaults, means the dataset is imbalanced, we need balanced.

# A) Balance the dataset
# sample() from pandas

# default_df = df[df['TARGET'] == 1]
# non_default_df = df[df['TARGET'] == 0].sample(n=len(default_df), random_state=42)

# balanced_df = pd.concat([default_df, non_default_df])

# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html

# print("Balanced data shape:", balanced_df.shape)

# B) Balance the dataset
#  resample() from sklearn.utils

# Separate classes
non_default = df[df['TARGET'] == 0]
default = df[df['TARGET'] == 1]

# Balance by undersampling the majority class
non_default_sample = resample(non_default,
                              replace=False,
                              n_samples=len(default),
                              random_state=42)

balanced_df = pd.concat([non_default_sample, default])
# https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html

print("Balanced data shape:", balanced_df.shape)

balanced_df.info()

# only 1052 rows, is too low data?

#
default_rate_after_balan = balanced_df['TARGET'].value_counts(normalize=True)[1] * 100
print(f"Default percentage after balancing: {default_rate_after_balan:.2f}%")

"""# 5. Plot the balanced or imbalanced data"""

# Plot the dataset balanced
sns.countplot(x='TARGET', data=balanced_df)
plt.title('Balanced Class Distribution')
plt.show()

"""# 6. Encode>Split>Scaler>PCA>Model(Keras.Sequencial)"""

# Encode categorical features
# A) labelEncoder

# for col in balanced_df.select_dtypes(include=['object']).columns:
#     balanced_df[col] = LabelEncoder().fit_transform(balanced_df[col])

# Encode categorical features
# B) get_dummies

# One-hot encode categorical variables
df_encoded = pd.get_dummies(balanced_df, drop_first=True)

df_encoded.info()

# Split features and target
X = df_encoded.drop('TARGET', axis=1)
y = df_encoded['TARGET']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. scaler features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply PCA
from sklearn.decomposition import PCA

pca = PCA(n_components=180)

# Aplica PCA Train
X_train_pca = pca.fit_transform(X_train_scaled)

# Transf pca test
X_test_pca = pca.transform(X_test_scaled)

pca = PCA()
pca.fit(X_train_scaled)

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid(True)
plt.show()

# Build deep learning model
# model = keras.Sequential([
#     layers.Input(shape=(X_train_scaled.shape[1],)),
#     layers.Dense(64, activation='relu'),
#     layers.Dropout(0.3),  # reduction overfitting
#     layers.Dense(32, activation='relu'),
#     layers.Dense(1, activation='sigmoid')  # binaria
# ])

# model = keras.Sequential([
#     layers.Input(shape=(X_train_pca.shape[1],)),  #  5 com
#     layers.Dense(64, activation='relu'),
#     layers.Dropout(0.3),
#     layers.Dense(32, activation='relu'),
#     layers.Dense(1, activation='sigmoid')
# ])

model = keras.Sequential([
    layers.Input(shape=(X_train_pca.shape[1],)),       # 100 PCA components
    layers.Dense(128, activation='relu'),              #
    layers.Dropout(0.3),
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')              #
])


# from tensorflow.keras.optimizers import Adam
from keras.optimizers import Adam

optimizer = Adam(learning_rate=0.0005)

# Compile model - AUC
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy', AUC(name='auc')]
)

early_stop = EarlyStopping(monitor='val_auc', patience=3, restore_best_weights=True, mode='max') #p=5 tested, before without mode='max'

# Train model
# history = model.fit(
#     X_train_scaled,
#     y_train,
#     epochs=20,
#     batch_size=32,
#     validation_data=(X_test_scaled, y_test),
#     callbacks=[early_stop]
# )


history = model.fit(
    X_train_pca,
    y_train,
    epochs=20,
    batch_size=32,
    validation_data=(X_test_pca, y_test),
    callbacks=[early_stop]
)

# Evaluate model
# y_pred_probs = model.predict(X_test_scaled).ravel()
# y_pred_class = (y_pred_probs > 0.5).astype(int)

y_pred_probs = model.predict(X_test_pca).ravel()
y_pred_class = (y_pred_probs > 0.5).astype(int)

# Metrics
accuracy = accuracy_score(y_test, y_pred_class)
recall = recall_score(y_test, y_pred_class)
roc_auc = roc_auc_score(y_test, y_pred_probs)

print("Model Evaluation:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Sensitivity (Recall): {recall:.2f}")
print(f"Area Under the ROC Curve (AUC): {roc_auc:.2f}") #ROC-AUC Score

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_class))

print("Classification Report:")
print(classification_report(y_test, y_pred_class))

# Plot ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""# Stratified K-Fold with Scalable PCA Tuning and Deep Learning Model"""

# from sklearn.model_selection import StratifiedKFold
# from sklearn.preprocessing import StandardScaler
# from sklearn.decomposition import PCA
# from keras.models import Sequential
# from keras.layers import Dense, Dropout, Input
# from keras.optimizers import Adam
# from keras.metrics import AUC
# from keras.callbacks import EarlyStopping
# import pandas as pd
# import numpy as np

# # Parameters
# pca_components_list = [140, 180, 200]
# n_splits = 5
# results = []

# # K-Fold loop
# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# for n_components in pca_components_list:
#     print(f"\n Testing PCA with {n_components} components...")
#     fold_metrics = []

#     for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
#         # Splitting
#         X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
#         y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

#         # Scaling
#         scaler = StandardScaler()
#         X_train_scaled = scaler.fit_transform(X_train)
#         X_val_scaled = scaler.transform(X_val)

#         # PCA
#         pca = PCA(n_components=n_components)
#         X_train_pca = pca.fit_transform(X_train_scaled)
#         X_val_pca = pca.transform(X_val_scaled)

#         # Model definition
#         model = Sequential([
#             Input(shape=(n_components,)),
#             Dense(128, activation='relu'),
#             Dropout(0.3),
#             Dense(64, activation='relu'),
#             Dense(32, activation='relu'),
#             Dense(1, activation='sigmoid')
#         ])

#         model.compile(
#             optimizer=Adam(learning_rate=0.0005),
#             loss='binary_crossentropy',
#             metrics=['accuracy', AUC(name='auc')]
#         )

#         # Early stopping
#         early_stop = EarlyStopping(
#             monitor='val_auc',
#             patience=3,
#             restore_best_weights=True,
#             mode='max'
#         )

#         # Training
#         history = model.fit(
#             X_train_pca, y_train,
#             validation_data=(X_val_pca, y_val),
#             epochs=20,
#             batch_size=32,
#             callbacks=[early_stop],
#             verbose=0
#         )

#         # Evaluation
#         scores = model.evaluate(X_val_pca, y_val, verbose=0)
#         fold_metrics.append({
#             'PCA': n_components,
#             'Fold': fold + 1,
#             'Accuracy': scores[1],
#             'AUC': scores[2]
#         })

#     results.extend(fold_metrics)

# # Final metrics overview
# metrics_df = pd.DataFrame(results)
# print("\n Consolidated Results:")
# print(metrics_df.groupby('PCA')[['Accuracy', 'AUC']].mean().round(4))

from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import (
    accuracy_score, recall_score, roc_auc_score,
    classification_report, confusion_matrix, roc_curve
)
from keras.models import Sequential
from keras.layers import Dense, Dropout, Input
from keras.optimizers import Adam
from keras.metrics import AUC
from keras.callbacks import EarlyStopping
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Parameters
pca_components_list = [140, 180, 200]
n_splits = 5
results = []

skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

for n_components in pca_components_list:
    print(f"\n Testing PCA with {n_components} components...")
    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        # Scaling and PCA
        scaler = StandardScaler().fit(X_train)
        X_train_scaled = scaler.transform(X_train)
        X_val_scaled = scaler.transform(X_val)

        pca = PCA(n_components=n_components).fit(X_train_scaled)
        X_train_pca = pca.transform(X_train_scaled)
        X_val_pca = pca.transform(X_val_scaled)

        # Model architecture
        model = Sequential([
            Input(shape=(n_components,)),
            Dense(128, activation='relu'),
            Dropout(0.3),
            Dense(64, activation='relu'),
            Dense(32, activation='relu'),
            Dense(1, activation='sigmoid')
        ])

        model.compile(
            optimizer=Adam(learning_rate=0.0005),
            loss='binary_crossentropy',
            metrics=['accuracy', AUC(name='auc')]
        )

        early_stop = EarlyStopping(
            monitor='val_auc',
            patience=3,
            restore_best_weights=True,
            mode='max'
        )

        # Train model
        history = model.fit(
            X_train_pca, y_train,
            validation_data=(X_val_pca, y_val),
            epochs=20,
            batch_size=32,
            callbacks=[early_stop],
            verbose=0
        )

        # Predict and Evaluate
        y_pred_probs = model.predict(X_val_pca).flatten()
        y_pred_class = (y_pred_probs >= 0.5).astype(int)

        acc = accuracy_score(y_val, y_pred_class)
        recall = recall_score(y_val, y_pred_class)
        auc = roc_auc_score(y_val, y_pred_probs)
        val_loss = history.history['val_loss'][-1]
        val_auc = history.history['val_auc'][-1]
        val_acc = history.history['val_accuracy'][-1]

        results.append({
            'PCA': n_components,
            'Fold': fold + 1,
            'Accuracy': acc,
            'Recall': recall,
            'AUC': auc,
            'Val_Accuracy': val_acc,
            'Val_AUC': val_auc,
            'Val_Loss': val_loss
        })

        # Print fold summary
        print(f"\n Fold {fold+1} — PCA {n_components}")
        print(f"Accuracy: {acc:.4f} | Recall: {recall:.4f} | AUC: {auc:.4f}")
        print("Confusion Matrix:")
        print(confusion_matrix(y_val, y_pred_class))
        print("Classification Report:")
        print(classification_report(y_val, y_pred_class))

# Aggregate Results
metrics_df = pd.DataFrame(results)
print("\n Consolidated Performance by PCA:")
print(metrics_df.groupby('PCA')[['Accuracy', 'Recall', 'AUC', 'Val_Accuracy', 'Val_AUC', 'Val_Loss']].mean().round(4))

avg_auc = metrics_df.groupby('PCA')['AUC'].mean()

plt.figure(figsize=(8, 6))
plt.plot(avg_auc.index, avg_auc.values, marker='o', color='teal')
plt.title('Mean AUC vs. Number of PCA Components')
plt.xlabel('PCA Components')
plt.ylabel('Mean AUC (Cross-Validation)')
plt.grid(True)
plt.show()

from joblib import dump
from keras.models import Sequential
from keras.layers import Dense, Dropout, Input
from keras.optimizers import Adam
from keras.metrics import AUC
from keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Preprocessing
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

pca = PCA(n_components=180).fit(X_train_scaled)
X_train_pca = pca.transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Model definition
model_final = Sequential([
    Input(shape=(180,)),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model_final.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss='binary_crossentropy',
    metrics=['accuracy', AUC(name='auc')]
)

# Early stopping
early_stop = EarlyStopping(
    monitor='auc',
    patience=3,
    restore_best_weights=True,
    mode='max'
)

# Final training
history = model_final.fit(
    X_train_pca, y_train,
    validation_data=(X_test_pca, y_test),
    epochs=20,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)


# Save pipeline components
dump(scaler, 'scaler.pkl')
dump(pca, 'pca_180.pkl')
model_final.save('model_final_180.keras')

# Pred
y_pred_probs = model_final.predict(X_test_pca)
y_pred_classes = (y_pred_probs > 0.5).astype(int)

# Metr
accuracy = accuracy_score(y_test, y_pred_classes)
recall = recall_score(y_test, y_pred_classes)
roc_auc = roc_auc_score(y_test, y_pred_probs)

print("\n Evaluacion:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Sensitivity (Recall): {recall:.2f}")
print(f"AUC: {roc_auc:.2f}")

#
print("\nClassification Report:")
print(classification_report(y_test, y_pred_classes))

#
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_classes))

fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""# Final Report

**Project Overview**

I worked with a dataset consisting of 307,511 records and 122 variables. My main goal was to apply a strategic and conservative data cleaning process—one that avoided losing valuable information through methods like dropna().
Through selective imputations, binary flag creation, and categorical standardization, I reshaped the dataset to contain 181 columns while preserving all original rows.

**Data Cleaning & Feature Engineering - Strategies Applied**

1. Numerical Variables with Low Missingness (≤ 1%)
- Examples: CNT_CHILDREN, AMT_CREDIT, DAYS_BIRTH
- Action: Imputation using median
- Reasoning: Preserves distribution and reduces the impact of outliers

2. Moderate Missingness (~10–20%)
- Key Features: EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3
- Action: Imputation using mean
- Reasoning: Features are normalized between 0 and 1, so the mean makes intuitive sense

3. Bureau Variables (AMT_REQ_CREDIT_BUREAU_*)
- Action: Impute with 0 + create *_missing_flag
- Reasoning: Missing values may reflect real behavioral patterns

4. High Missingness (>50%)
- Examples: COMMONAREA_MEDI, OWN_CAR_AGE
- Action: Median imputation + binary flags
- Reasoning: Absence itself might hold predictive value

5. Categorical Features with Light Missingness
- Examples: NAME_TYPE_SUITE, ORGANIZATION_TYPE
- Action: Imputation with mode
- Reasoning: Keeps category distributions intact

6. Categorical Features with Heavy Missingness
- Examples: OCCUPATION_TYPE, WALLSMATERIAL_MODE
- Action: Imputation using "unknown" + creation of missing flags
- Reasoning: Helps treat missingness as a potentially meaningful signal

7. String Standardization
- Examples: CODE_GENDER, FLAG_OWN_CAR
- Action: Applied .str.lower().strip()
- Reasoning: Prevents duplication during encoding

Additional Feature Engineering

| *_missing_flag | Allows model to learn from explicit absence |

| Dropping SK_ID_CURR | Removed due to lack of predictive value |

**Data Balancing Strategy**

To address class imbalance (only 8.07% defaults), I used undersampling with resample() from sklearn.utils.

```
non_default = df[df['TARGET'] == 0]
default = df[df['TARGET'] == 1]

non_default_sample = resample(
    non_default,
    replace=False,
    n_samples=len(default),
    random_state=42
)

balanced_df = pd.concat([non_default_sample, default])
```

Outcome

- Final shape: (49,650, 181)
- Data types: float64 (65), int64 (100), object (16)

Encoding:

```
df_encoded = pd.get_dummies(balanced_df, drop_first=True)
```

Outcome

- 49650 entries
- Columns: 293

**PCA and Deep Learning Evaluation**

1. Dimensionality Reduction with PCA

Principal Component Analysis (PCA) was implemented to reduce the original feature space from 293 variables to a more compact representation ranging from 5 to 140/180/200 components. This helped simplify the model, reduce overfitting risk, and maintain meaningful signal.

Key Observations:

- PCA with 180 components provided the best balance between performance and stability.

- The validation AUC improved across component sizes, but diminishing returns were noticeable above 180.

- PCA transformed the input data into smooth, continuous features well-suited for neural networks.


2. Deep Learning Model with PCA Inputs

A sequential model using ReLU activations and dropout regularization was trained on PCA-transformed data. It included layers with 128, 64, and 32 neurons followed by a sigmoid output.

Configuration:

- Optimizer: Adam (lr = 0.0005)
- EarlyStopping: Patience = 3, monitored val_auc
- Batch size: 32
- Epochs: 20

Final Metrics (PCA = 200):
- Accuracy: 0.68
- Sensitivity (Recall): 0.69
- Area Under the ROC Curve (AUC): 0.74

This model achieved high recall, making it suitable for risk-related classification tasks where false negatives are costly.

**Stratified K-Fold Validation with Scalable PCA Tuning**

To confirm generalization, a Stratified K-Fold loop with 5 folds was implemented. Each fold involved:

- Standard scaling
- PCA transformation
- Model training with early stopping
- Evaluation using accuracy, recall, AUC, and confusion matrix
Cross-Validated Comparison:

| PCA Components | Accuracy | Recall | AUC | Val_Loss |

| 140 | 0.67 | 0.64 | 0.73 | 0.61 |

| 180 | 0.68 | 0.69 | 0.74 | 0.60 |

| 200 | 0.68 | 0.67 | 0.74 | 0.60 |
"""