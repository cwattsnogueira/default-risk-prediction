# -*- coding: utf-8 -*-
"""DeepLearningwithTensorFlowandKeras:CourseEndProjectsV2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13yA7oj-B65r3Hek-7DItH8QVTrVzOgkb

# Deep Learning with TensorFlow and Keras: Course End Projects

https://drive.google.com/drive/folders/1kwwdDkY5QYqpS_OwB7DOm1GJ7Mox92nb

Carllos Watts-Nogueira

Due: Jul 12 by 12:59am

# Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, recall_score, classification_report, roc_curve, roc_auc_score

from sklearn.utils import resample

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.metrics import AUC

from keras.callbacks import EarlyStopping

"""# 1. and 2. Load dataset / Check for null values in the dataset"""

# Load the Data Dic
df_dd = pd.read_csv('Data_Dictionary.csv')
df_dd

df_dd.info()

# Load dataset
df = pd.read_csv('loan_data.csv')

# copy df
df_main_cpy = df.copy(deep=True)

df_main_cpy.info()

df.info()

df.head()

df.isnull().sum()

# dtype
df.dtypes

# new df
df_info = pd.DataFrame({
    'Column Name': df.columns,
    'Data Type': df.dtypes.values,
    'Missing Values': df.isnull().sum().values,
    'Missing %': (df.isnull().mean().values * 100).round(2)
})

# mix by column name
summary_df = pd.merge(df_info, df_dd, how='left', left_on='Column Name', right_on='Variable Name')

# Select columns, rename
summary_df = summary_df[['Column Name', 'Description', 'Data Type', 'Missing Values', 'Missing %']]

# show all rows
pd.set_option('display.max_rows', None)

# print
summary_df

summary_df[summary_df['Data Type'] == 'object']

# Cria uma cópia do dataframe apenas com as colunas do tipo 'object'
object_summary = summary_df[summary_df['Data Type'].astype(str) == 'object'].copy()

# Adiciona a nova coluna com os valores únicos de cada coluna
object_summary['Unique Values'] = object_summary['Column Name'].apply(lambda col: df[col].dropna().unique().tolist())

object_summary

summary_df[summary_df['Missing %'] > 50].sort_values('Missing %', ascending=False)

print(summary_df['Data Type'].unique())

summary_df[summary_df['Data Type'].astype(str).isin(['int64', 'float64'])]

nulls = df.isnull().sum()
print(nulls[nulls > 0])

medium_missing = nulls[(nulls > 0) & (nulls < 41519)]
print(medium_missing)

high_missing = nulls[nulls > 41519]
print(high_missing)

# copy df
df_main = df.copy(deep=True)

"""# Data Cleaning + Feature Engineering"""

# Variables with few missing values (up to 1%)

# Simple imputation for low-missing variables
low_nulls = ['CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',
             'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH',
             'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH',
             'CNT_FAM_MEMBERS']

for col in low_nulls:
    df[col] = df[col].fillna(df[col].median())

# variables with moderate missing rates (10-20%)

# Impute EXT_SOURCE columns with mean
ext_sources = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']
for col in ext_sources:
    df[col] = df[col].fillna(df[col].mean())

# Create missing flags for bureau-related features
bureau_cols = [col for col in df.columns if 'AMT_REQ_CREDIT_BUREAU' in col]
for col in bureau_cols:
    df[col + '_missing_flag'] = df[col].isnull().astype(int)
    df[col] = df[col].fillna(0)

# Structural/physical variables (~50–70% missing)

# Identify numeric columns with more than 50% missing values
high_missing_numeric = [
    col for col in df.select_dtypes(include=['number']).columns
    if df[col].isnull().mean() > 0.5
]

# Efficient creation of missing flags
missing_flags = {
    col + '_missing_flag': df[col].isnull().astype(int)
    for col in high_missing_numeric
}

# Concatenate all flags into the main DataFrame
df = pd.concat([df, pd.DataFrame(missing_flags)], axis=1)

# Impute missing values using the median
for col in high_missing_numeric:
    df[col] = df[col].fillna(df[col].median())

# Normalize strings: categorical columns with no missing values
categorical_cols = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']
for col in categorical_cols:
    df[col] = df[col].str.lower().str.strip()

# Impute mode for columns with low missingness
light_nulls = [
    'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',
    'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'WEEKDAY_APPR_PROCESS_START',
    'ORGANIZATION_TYPE'
]
for col in light_nulls:
    df[col] = df[col].fillna(df[col].mode()[0])

# Impute 'Unknown' for categorical columns with high missingness
heavy_nulls = [
    'OCCUPATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE',
    'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE'
]
for col in heavy_nulls:
    df[col + '_missing_flag'] = df[col].isnull().astype(int)
    df[col] = df[col].fillna('Unknown')

# new df
df_infotw = pd.DataFrame({
    'Column Name': df.columns,
    'Data Type': df.dtypes.values,
    'Missing Values': df.isnull().sum().values,
    'Missing %': (df.isnull().mean().values * 100).round(2)
})

# mix dic by colum name
summary_df = pd.merge(df_infotw, df_dd, how='left', left_on='Column Name', right_on='Variable Name')

# rename
summary_df = summary_df[['Column Name', 'Description', 'Data Type', 'Missing Values', 'Missing %']]

# all rows
pd.set_option('display.max_rows', None)

#
summary_df

# Identify columns where nulls are still present
null_summary = df.isnull().sum()
remaining_nulls = null_summary[null_summary > 0]
print(remaining_nulls)

low_nulls = df.columns[df.isnull().sum() <= 2].tolist()

for col in low_nulls:
    if pd.api.types.is_numeric_dtype(df[col]):
        df[col] = df[col].fillna(df[col].median())
    else:
        df[col] = df[col].fillna(df[col].mode()[0])

# List of relevant columns to treat
relevant_nulls = [
    'YEARS_BEGINEXPLUATATION_AVG', 'FLOORSMAX_AVG',
    'YEARS_BEGINEXPLUATATION_MODE', 'FLOORSMAX_MODE',
    'YEARS_BEGINEXPLUATATION_MEDI', 'FLOORSMAX_MEDI',
    'TOTALAREA_MODE', 'OBS_30_CNT_SOCIAL_CIRCLE',
    'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',
    'DEF_60_CNT_SOCIAL_CIRCLE', 'NAME_CONTRACT_TYPE'  # example non-numeric
]

# Create missing flags efficiently
missing_flags = {
    col + '_missing_flag': df[col].isnull().astype(int)
    for col in relevant_nulls
}

# Add flags to the main DataFrame
df = pd.concat([df, pd.DataFrame(missing_flags)], axis=1)

# Impute based on column type
for col in relevant_nulls:
    if pd.api.types.is_numeric_dtype(df[col]):
        df[col] = df[col].fillna(df[col].median())
    else:
        df[col] = df[col].fillna(df[col].mode(dropna=True)[0])

print("Remaining nulls:", df.isnull().sum().sum())

#
df_info = pd.DataFrame({
    'Column Name': df.columns,
    'Data Type': df.dtypes.values,
    'Missing Values': df.isnull().sum().values,
    'Missing %': (df.isnull().mean().values * 100).round(2)
})

#
summary_df = pd.merge(df_info, df_dd, how='left', left_on='Column Name', right_on='Variable Name')

#
summary_df = summary_df[['Column Name', 'Description', 'Data Type', 'Missing Values', 'Missing %']]

#
pd.set_option('display.max_rows', None)

#
summary_df

df = df.drop(columns=['SK_ID_CURR'])

# Drop duplicate or malformed flag columns
columns_to_drop = [col for col in df.columns if 'missing_flag_missing_flag' in col]
df = df.drop(columns=columns_to_drop)

missing_percentage = df.isnull().mean() * 100
missing_percentage

# df.dropna(inplace=True)

df.info()

# is ok I deleted 298.909 is ok? or better dropna() columns more afected with nulls and latter dropna rows?

df.columns

"""# 3. Print the percentage of default to a payer of the dataset for the TARGET column"""

# Option A, with mean()
# default_rate = df['TARGET'].mean()
# print(f"Default percentage: {default_rate * 100:.2f}%")

# Option B, with value_counts and normalize
default_rate = df['TARGET'].value_counts(normalize=True)[1] * 100
print(f"Default percentage: {default_rate:.2f}%")

"""# 4. Balance the dataset if the data is imbalanced"""

# data is imbalanced?
sns.countplot(x='TARGET', data=df)
plt.title('Default vs Non-default Counts')
plt.show()

# If the data is more "0" non-defaults than "1" defaults, means the dataset is imbalanced, we need balanced.

# A) Balance the dataset
# sample() from pandas

# default_df = df[df['TARGET'] == 1]
# non_default_df = df[df['TARGET'] == 0].sample(n=len(default_df), random_state=42)

# balanced_df = pd.concat([default_df, non_default_df])

# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html

# print("Balanced data shape:", balanced_df.shape)

# B) Balance the dataset
#  resample() from sklearn.utils

# Separate classes
non_default = df[df['TARGET'] == 0]
default = df[df['TARGET'] == 1]

# Balance by undersampling the majority class
non_default_sample = resample(non_default,
                              replace=False,
                              n_samples=len(default),
                              random_state=42)

balanced_df = pd.concat([non_default_sample, default])
# https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html

print("Balanced data shape:", balanced_df.shape)

balanced_df.info()

# only 1052 rows, is too low data?

#
default_rate_after_balan = balanced_df['TARGET'].value_counts(normalize=True)[1] * 100
print(f"Default percentage after balancing: {default_rate_after_balan:.2f}%")

"""# 5. Plot the balanced or imbalanced data"""

# Plot the dataset balanced
sns.countplot(x='TARGET', data=balanced_df)
plt.title('Balanced Class Distribution')
plt.show()

"""# 6. Encode the columns that are required for the model"""

# Encode categorical features
# A) labelEncoder

# for col in balanced_df.select_dtypes(include=['object']).columns:
#     balanced_df[col] = LabelEncoder().fit_transform(balanced_df[col])

# Encode categorical features
# B) get_dummies

# One-hot encode categorical variables
df_encoded = pd.get_dummies(balanced_df, drop_first=True)

df_encoded.info()

# Split features and target
X = df_encoded.drop('TARGET', axis=1)
y = df_encoded['TARGET']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. scaler features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build deep learning model
model = keras.Sequential([
    layers.Input(shape=(X_train_scaled.shape[1],)),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),  # reduction overfitting
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # binaria
])

# Compile model - AUC
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy', AUC(name='auc')]
)

early_stop = EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True)

# Train model
history = model.fit(
    X_train_scaled,
    y_train,
    epochs=20,
    batch_size=32,
    validation_data=(X_test_scaled, y_test),
    callbacks=[early_stop]
)

# Evaluate model
y_pred_probs = model.predict(X_test_scaled).ravel()
y_pred_class = (y_pred_probs > 0.5).astype(int)

# Metrics
accuracy = accuracy_score(y_test, y_pred_class)
recall = recall_score(y_test, y_pred_class)
roc_auc = roc_auc_score(y_test, y_pred_probs)

print("Model Evaluation:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Sensitivity (Recall): {recall:.2f}")
print(f"Area Under the ROC Curve (AUC): {roc_auc:.2f}") #ROC-AUC Score

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_class))

print("Classification Report:")
print(classification_report(y_test, y_pred_class))

# Plot ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""# Final Report

**Project Overview**

I worked with a dataset consisting of 307,511 records and 122 variables. My main goal was to apply a strategic and conservative data cleaning process—one that avoided losing valuable information through methods like dropna().
Through selective imputations, binary flag creation, and categorical standardization, I reshaped the dataset to contain 181 columns while preserving all original rows.

**Data Cleaning & Feature Engineering - Strategies Applied**

1. Numerical Variables with Low Missingness (≤ 1%)
- Examples: CNT_CHILDREN, AMT_CREDIT, DAYS_BIRTH
- Action: Imputation using median
- Reasoning: Preserves distribution and reduces the impact of outliers

2. Moderate Missingness (~10–20%)
- Key Features: EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3
- Action: Imputation using mean
- Reasoning: Features are normalized between 0 and 1, so the mean makes intuitive sense

3. Bureau Variables (AMT_REQ_CREDIT_BUREAU_*)
- Action: Impute with 0 + create *_missing_flag
- Reasoning: Missing values may reflect real behavioral patterns

4. High Missingness (>50%)
- Examples: COMMONAREA_MEDI, OWN_CAR_AGE
- Action: Median imputation + binary flags
- Reasoning: Absence itself might hold predictive value

5. Categorical Features with Light Missingness
- Examples: NAME_TYPE_SUITE, ORGANIZATION_TYPE
- Action: Imputation with mode
- Reasoning: Keeps category distributions intact

6. Categorical Features with Heavy Missingness
- Examples: OCCUPATION_TYPE, WALLSMATERIAL_MODE
- Action: Imputation using "unknown" + creation of missing flags
- Reasoning: Helps treat missingness as a potentially meaningful signal

7. String Standardization
- Examples: CODE_GENDER, FLAG_OWN_CAR
- Action: Applied .str.lower().strip()
- Reasoning: Prevents duplication during encoding

Additional Feature Engineering

| *_missing_flag | Allows model to learn from explicit absence |

| Dropping SK_ID_CURR | Removed due to lack of predictive value |

**Data Balancing Strategy**

To address class imbalance (only 8.07% defaults), I used undersampling with resample() from sklearn.utils.

```
non_default = df[df['TARGET'] == 0]
default = df[df['TARGET'] == 1]

non_default_sample = resample(
    non_default,
    replace=False,
    n_samples=len(default),
    random_state=42
)

balanced_df = pd.concat([non_default_sample, default])
```

Outcome

- Final shape: (49,650, 181)
- Data types: float64 (65), int64 (100), object (16)

Encoding:

```
df_encoded = pd.get_dummies(balanced_df, drop_first=True)
```

Outcome

- 49650 entries
- Columns: 293

**Modeling – Deep Learning**

**Architecture**


```
model = keras.Sequential([
    layers.Input(shape=(X_train_scaled.shape[1],)),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])
```
- Optimizer: Adam
- Loss Function: Binary Crossentropy
- Metrics: Accuracy, AUC


**Training**

- 20 epochs with EarlyStopping on val_auc
- Best performance around epoch 8
- Stable validation AUC ≈ 0.743

Final Evaluation
| Metric | Value |

| Accuracy | 0.68 |

| Recall | 0.67 |

| AUC ROC | 0.74 |

| Threshold | 0.5 |

**Confusion Matrix**
[[3422 1531]
 [1636 3341]]


**Classification Report**
| Class | Precision | Recall | F1-score | Support |

| 0 | 0.68 | 0.69 | 0.68 | 4953 |

| 1 | 0.69 | 0.67 | 0.68 | 4977 |

| Macro Avg | 0.68 | 0.68 | 0.68 | 9930 |

| Weighted Avg | 0.68 | 0.68 | 0.68 | 9930 |


**Conclusion**

This project led me to question the choices I made: was the approach of selective imputation and explicit flag creation truly the most appropriate for handling missing data? Could there be more effective practices out there?
How can we be guided toward a more robust and generalizable solution, especially in the face of severe class imbalance?
The results, when compared to the earlier strategy of dropping incomplete records, showed significant improvement—but does this alone validate the method as reliable and replicable?
Can this work be considered solid within the standards of data science best practices, or are there gaps that still need to be addressed?
"""